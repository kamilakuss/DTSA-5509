{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd155f25",
   "metadata": {},
   "source": [
    "# Forecasting Supermarket Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ec99d",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34421f",
   "metadata": {},
   "source": [
    "For this project, I am aiming to predict sales based on historical data using supervised machine learning technique XG regression. This is a supermarket chain with 54 locations across Ecuador. Given the data from 2013 to 2017, the problem is to forecast the sales for 2 weeks in 2017. This is a regression problem, since I will be trying to predict a continuous value. It is important to be able to forecast sales to order accordingly, minimizing wasted products and increasing profits. <br>\n",
    "\n",
    "All of the datasets came from Kaggle. I will be using the data_train dataset to split into training and testing data using 80/20 split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee14700",
   "metadata": {},
   "source": [
    "Citations:\n",
    "Kaggle. N/A. Store Sales - Time Series Forecasting, Version 1. Retrieved Aug 5, 2024 from https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9787d7c5-d91b-40d9-b01e-b9bf059391c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e161b1",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e9fb11d-7e1b-47bf-8ae3-939ae0d1ff8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data_oil (1218, 2)\n",
      "column types in data_oil :\n",
      "date           object\n",
      "dcoilwtico    float64\n",
      "dtype: object\n",
      "There are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01         NaN\n",
       "1  2013-01-02       93.14\n",
       "2  2013-01-03       92.97\n",
       "3  2013-01-04       93.12\n",
       "4  2013-01-07       93.20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "shape of data_holidays (350, 6)\n",
      "column types in data_holidays :\n",
      "date           object\n",
      "type           object\n",
      "locale         object\n",
      "locale_name    object\n",
      "description    object\n",
      "transferred      bool\n",
      "dtype: object\n",
      "There are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Manta</td>\n",
       "      <td>Fundacion de Manta</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Cotopaxi</td>\n",
       "      <td>Provincializacion de Cotopaxi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Fundacion de Cuenca</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Libertad</td>\n",
       "      <td>Cantonizacion de Libertad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-21</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Riobamba</td>\n",
       "      <td>Cantonizacion de Riobamba</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     type    locale locale_name                    description  \\\n",
       "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
       "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
       "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
       "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
       "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
       "\n",
       "   transferred  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "shape of data_stores (54, 5)\n",
      "column types in data_stores :\n",
      "store_nbr     int64\n",
      "city         object\n",
      "state        object\n",
      "type         object\n",
      "cluster       int64\n",
      "dtype: object\n",
      "There are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state type  cluster\n",
       "0          1          Quito                       Pichincha    D       13\n",
       "1          2          Quito                       Pichincha    D       13\n",
       "2          3          Quito                       Pichincha    D        8\n",
       "3          4          Quito                       Pichincha    D        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "shape of data_train (3000888, 6)\n",
      "column types in data_train :\n",
      "id               int64\n",
      "date            object\n",
      "store_nbr        int64\n",
      "family          object\n",
      "sales          float64\n",
      "onpromotion      int64\n",
      "dtype: object\n",
      "There are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  store_nbr      family  sales  onpromotion\n",
       "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
       "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
       "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
       "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
       "4   4  2013-01-01          1       BOOKS    0.0            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "shape of data_transactions (83488, 3)\n",
      "column types in data_transactions :\n",
      "date            object\n",
      "store_nbr        int64\n",
      "transactions     int64\n",
      "dtype: object\n",
      "There are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>3487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr  transactions\n",
       "0  2013-01-01         25           770\n",
       "1  2013-01-02          1          2111\n",
       "2  2013-01-02          2          2358\n",
       "3  2013-01-02          3          3487\n",
       "4  2013-01-02          4          1922"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the data \n",
    "data_oil = pd.read_csv('raw_data/oil.csv')\n",
    "data_holidays = pd.read_csv('raw_data/holidays_events.csv')\n",
    "data_stores = pd.read_csv('raw_data/stores.csv')\n",
    "data_train = pd.read_csv('raw_data/train.csv')\n",
    "data_transactions = pd.read_csv('raw_data/transactions.csv')\n",
    "\n",
    "#print out general info abput the datasets\n",
    "names = ['data_oil', 'data_holidays', 'data_stores', 'data_train', 'data_transactions']\n",
    "for name, data in zip(names, [data_oil, data_holidays, data_stores, data_train, data_transactions]):\n",
    "    print('shape of', name, data.shape)\n",
    "    #print('columns in', name , ':', data.columns )\n",
    "    print ('column types in', name ,':')\n",
    "    print(data.dtypes)\n",
    "    missing_values = data_train[data_train.isnull().any(axis=1)]\n",
    "    if missing_values.empty:\n",
    "        print('There are no missing values')\n",
    "    else:\n",
    "        print(missing_values)\n",
    "    display(data.head())\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522c2a0",
   "metadata": {},
   "source": [
    "In addition to the information above, here are the data colunm descriptions: <br>\n",
    "<br>\n",
    "for data_oil : <br>\n",
    "date indicates the date <br>\n",
    "dcoilwtico indicates the oil price on that date <br>\n",
    "<br>\n",
    "\n",
    "for data_holidays <br>\n",
    "date indicates the date <br>\n",
    "type indicates whether it a aholiday or an event <br>\n",
    "locale tracks whether it is a national , regional or local holiday/event <br>\n",
    "locale_name tracks where the evnt is taking place<br>\n",
    "description tracks the name of the holidya/event<br>\n",
    "transferred tracks whether the holday/event was moved from a holiday to the next weekday<br>\n",
    "<br>\n",
    "\n",
    "for data_stores<br>\n",
    "store_nbr tracks the assigned store number<br>\n",
    "city indicates the city in which that store is located<br>\n",
    "state indicates the state in which that store is located<br>\n",
    "type indicates the type of store<br>\n",
    "cluster indicates the store cluster<br>\n",
    "\n",
    "\t\n",
    "for data_train<br>\n",
    "id is the id of the observation<br>\n",
    "date indicates the date<br>\n",
    "store_nbr indicates the store number (same as in data_stores)<br>\n",
    "family indicates the item family <br>\n",
    "sales tracks the sales for that particular item family on that date at that store<br>\n",
    "onpromotion indicates how many items are on promotion at that store<br>\n",
    "<br>\n",
    "for data_transactions:<br>\n",
    "date indicates the date<br>\n",
    "store_nbr indicates the store number (same as in data_stores and data_train)<br>\t\n",
    "transactions indicates the number of transactions on that date at that store.<br>\n",
    "<br>\n",
    "Above are the data descriptions, including the number of rows, number of columns, data type of each feature , description of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7cd5c7e-e0fe-4fe8-b901-4a3581d728f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing data_oil ...\n",
      "\n",
      "processing data_holidays ...\n",
      "\n",
      "processing data_stores ...\n",
      "the date column does not exist in data_stores\n",
      "\n",
      "processing data_train ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create columns for year, month, date, day of week for easier aggregation\n",
    "for name, data in zip(names, [data_oil, data_holidays, data_stores,  data_train, data_transactions]):\n",
    "    print()\n",
    "    print('processing', name, '...')\n",
    "    if 'date' in data.columns:\n",
    "        #Convert from object to datetime \n",
    "        data.date = pd.to_datetime(data.date)\n",
    "        data['year'] = data.date.dt.year\n",
    "        data['month'] = data.date.dt.month\n",
    "        data['day'] = data.date.dt.day\n",
    "        data['day_of_week'] = data.date.dt.dayofweek\n",
    "        data['day_name'] = data.date.dt.strftime('%A')\n",
    "    \n",
    "    else:\n",
    "        print('the date column does not exist in', name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6327e0c-0580-45bc-a969-0fadbed60665",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#merge the data\n",
    "\n",
    "#Clone the df to compare after merging\n",
    "data_train_original = data_train\n",
    "\n",
    "#Merge the oil data \n",
    "data_train = pd.merge(data_train, data_oil[['date', 'dcoilwtico']], on='date', how='left')\n",
    "\n",
    "#Merge holidays data\n",
    "data_holidays.rename(columns={'type': 'holiday_type'}, inplace=True)\n",
    "data_train = pd.merge(data_train, data_holidays, on = ['date', 'day', 'month', 'year', 'day_of_week', 'day_name'], how = 'left')\n",
    "\n",
    "#Merge the store data\n",
    "data_stores.rename(columns={'type': 'store_type'}, inplace=True)\n",
    "data_train = pd.merge(data_train, data_stores, on='store_nbr', how='left')\n",
    "\n",
    "#Merge the transactions data\n",
    "data_train = pd.merge(data_train, data_transactions, on = ['date', 'store_nbr', 'day', 'month', 'year', 'day_of_week', 'day_name'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e217f6-7cf4-4faa-a078-401db6213049",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('The number of rows increased from', data_train_original.shape[0], 'to', data_train.shape[0], 'due to local and national holiday overlap.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb461f6-f348-4dcb-a057-9fbb7851e5db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# How many unique stores are there in the training set?\n",
    "print('There are', len(data_train['store_nbr'].unique()) , 'unique store IDs in the training dataset.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba9c5-e120-41ac-980b-fb00c626cac6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Is there any difference between stores closer to the eartquake vs further?\n",
    "\n",
    "print('There are',len(data_stores.state.unique()), 'unique values in the state column.')\n",
    "\n",
    "# According to worldvision.org these states were affected the most by the earthquake\n",
    "worst_affected_states= ['Manabi', 'Esmeraldas', 'Sanat Elena', 'Guayas', 'Santo Domingo de los Tsachilas', 'Los Rios']\n",
    "print('There are ',len(data_stores.city.unique()), 'unique values in the city column')\n",
    "\n",
    "# Find the cities that are located in these states\n",
    "worst_affected_cities = data_stores[data_stores['state'].isin(worst_affected_states)]['city'].unique().tolist()\n",
    "\n",
    "#Find the stores taht are located in these states\n",
    "worst_affected_stores = data_stores[data_stores['state'].isin(worst_affected_states)]['store_nbr'].unique().tolist()\n",
    "print('These are the states that were affected by the earthquake the most: ', worst_affected_states)\n",
    "print('These are the cities that were affected by the earthquake the most: ', worst_affected_cities)\n",
    "print('These are the ', len(worst_affected_stores), 'stores that were affected by the earthquake the most: ', worst_affected_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e354b",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0a6ed-2b72-4b6a-b323-1a16d00a48ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#What are the year-to-year trends in the most affected stores vs others vs the mean?\n",
    "\n",
    "fig = plt.figure(figsize=(13, 25))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "\n",
    "# Initialize a list to collect legend handles and labels\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, product in enumerate(data_train.family.unique()):\n",
    "    ax = fig.add_subplot(11, 3, i + 1)\n",
    "    \n",
    "    # Plot for worst-affected stores (red color)\n",
    "    data_train[data_train['store_nbr'].isin(worst_affected_stores)].query('family == @product').groupby(['year']).sales.mean().plot(ax=ax, color='red', label='Worst Affected Stores')\n",
    "    \n",
    "    # Plot for non-affected stores (blue color)\n",
    "    data_train[~data_train['store_nbr'].isin(worst_affected_stores)].query('family == @product').groupby(['year']).sales.mean().plot(ax=ax, color='blue', label='Non-Affected Stores')\n",
    "    \n",
    "    # Plot for all stores combined (grey color)\n",
    "    data_train.query('family == @product').groupby(['year']).sales.mean().plot(ax=ax, color='grey', label='All Stores', linestyle='--')\n",
    "    \n",
    "    # Set the title of the subplot\n",
    "    plt.title(product)\n",
    "    \n",
    "    # Collect the handles and labels only once (from the first subplot)\n",
    "    if i == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Create a single legend outside the subplots\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=3, frameon=False)\n",
    "\n",
    "plt.savefig('plot1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906bab8",
   "metadata": {},
   "source": [
    "As can be seen from the graph above, the stores located in the areas affected most by the 2016 earthquake (red) have been historically performing worse than the rest (blue). Most of them follow the same sales pattern but on a smaller scale. Most item families have a generally upward growth trend over the years, with the notable exception being lingerie which hit the minimum in 2016 but recovered in 2017 and frozen foods, since the 2017 dataset does not include december, the highest grossing month in frozen food sales, as will be demonstrated shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889128f6-d162-440f-9ace-df3070751151",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#What are the trends month-by-month each year?\n",
    "fig = plt.figure(figsize=(13, 25))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "\n",
    "# Initialize a list to collect legend handles and labels\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, product in enumerate(data_train.family.unique()):\n",
    "    ax = fig.add_subplot(11, 3, i + 1)\n",
    "    \n",
    "    select = data_train.query('family==@product')\n",
    "    for year in [2013,2014,2015,2016,2017]: \n",
    "        select.query('year==@year').groupby(['month']).sales.mean().plot(ax=ax,label=year)\n",
    "\n",
    "    plt.title(product)\n",
    "    \n",
    "    # Collect the handles and labels only once (from the first subplot)\n",
    "    if i == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Create a single legend outside the subplots\n",
    "fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.05), ncol=3, frameon=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02de96b",
   "metadata": {},
   "source": [
    "Liquor, wine, beer' , 'Frozen foods', 'Grocery I',  'Grocery II', and 'Home and Kitchen I' seem to spike in december every year. In specific categories the sales are at 0 the whole year. With most of them being in 2013, it is likely that either those stores did not carry those items at the time, or this is a tracking or entry error. Thus, I will remove them from the dataset. <br>\n",
    "In 2014 the data is spotty, with the months 2, 4, 5, 6, 8 presenting 0 sales in 11 different categories. This is likely an error as well. I will duplicate the dataset before imputing these values. I will replace the missing values with the mean sales in that family for the other months of the year.  <br>\n",
    "In 2017 there are spikes in 'Lawn and Garden' and 'School and office supplies'. I will analyze whether these values are outliers by examining whether the sales happen in one location or in different ones.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a5dce-f7fa-4d54-916d-26417d9d6b90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Remove the years with 0 sales \n",
    "#Create a new training df that drops null values\n",
    "\n",
    "#Identify the categories and years in which the wjole year the sales are 0\n",
    "zero_sales = data_train.groupby(['family', 'year'])['sales'].sum().reset_index()\n",
    "#display(zero_sales)\n",
    "zero_sales = zero_sales.query('sales==0')\n",
    "display(zero_sales)\n",
    "print('There are',len(zero_sales), 'categories in which for a whole year the sales were 0. We will drop those observations.')\n",
    "\n",
    "#Create a boolean mask for groups with zero total sales\n",
    "zero_sales_mask = data_train.groupby(['family', 'year'])['sales'].transform('sum') == 0\n",
    "\n",
    "#Filter out the rows where the mask is True\n",
    "data_train_filtered = data_train[~zero_sales_mask]\n",
    "\n",
    "#Convert year back to int\n",
    "data_train_filtered = data_train_filtered.copy()\n",
    "data_train_filtered['year'] = data_train_filtered['year'].astype(int)\n",
    "\n",
    "print('To check that they were removed correctly, check the shapes of the original data-train and the new dataframe. Original shape:', data_train.shape, 'New shape:', data_train_filtered.shape,'.', (data_train.shape[0] - data_train_filtered.shape[0]) ,'observations were removed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the unfiltered dataset to compare the accuracy\n",
    "data1 = data_train_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the sales for 11 categoriwes in months 2, 4, 5, 6, 8 in 2014 with the mean sales\n",
    "# Filter data for 2014 only\n",
    "data_2014 = data_train_filtered[data_train_filtered['year'] == 2014]\n",
    "\n",
    "# Define the affected months and categories\n",
    "affected_months = [2, 4, 5, 6, 8]\n",
    "affected_categories = ['BABY CARE', 'BEVERAGES', 'CELEBRATION', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II', \n",
    "                       'HOME CARE', 'LADIESWEAR', 'MAGAZINES', 'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'PRODUCE']\n",
    "\n",
    "# Create a mask to identify the affected rows\n",
    "mask = (data_2014['month'].isin(affected_months)) & (data_2014['family'].isin(affected_categories)) & (data_2014['sales'] == 0)\n",
    "\n",
    "# Subset the affected data\n",
    "affected_data = data_2014.loc[mask]\n",
    "\n",
    "# Calculate the mean sales for each family in unaffected months (excluding 2, 4, 5, 6, 8)\n",
    "mean_sales_other_months = data_2014[~data_2014['month'].isin(affected_months)] \\\n",
    "    .groupby(['family'])['sales'].mean().reset_index().rename(columns={'sales': 'mean_sales'})\n",
    "\n",
    "# Merge the mean sales back to the affected data\n",
    "affected_data_with_means = affected_data.merge(mean_sales_other_months, on='family', how='left')\n",
    "\n",
    "# Replace the sales values in data_train_filtered with the mean sales from other months\n",
    "# Ensure the index remains aligned by using the `affected_data.index`\n",
    "data_train_filtered.loc[affected_data.index, 'sales'] = affected_data_with_means['mean_sales'].values\n",
    "\n",
    "# Display the affected rows before and after replacement for verification\n",
    "comparison_df = affected_data_with_means[['family', 'store_nbr', 'month', 'sales', 'mean_sales']].copy()\n",
    "comparison_df['sales_before'] = 0  # Since sales were 0 in the affected rows\n",
    "comparison_df['sales_after'] = affected_data_with_means['mean_sales'].values\n",
    "\n",
    "print(comparison_df[['family', 'store_nbr', 'month', 'sales_before', 'sales_after']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179ef2b-be62-4654-93d5-945a3512318c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Display the month-to-month graphs after removing the zero-sale years\n",
    "\n",
    "fig = plt.figure(figsize=(13, 25))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "\n",
    "# Initialize a list to collect legend handles and labels\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, product in enumerate(data_train.family.unique()):\n",
    "    ax = fig.add_subplot(11, 3, i + 1)\n",
    "    \n",
    "    select = data_train_filtered.query('family==@product')\n",
    "    for year in [2013,2014,2015,2016,2017]: \n",
    "        select.query('year==@year').groupby(['month']).sales.mean().plot(ax=ax,label=year)\n",
    "\n",
    "    plt.title(product)\n",
    "    \n",
    "    # Collect the handles and labels only once (from the first subplot)\n",
    "    if i == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Create a single legend outside the subplots\n",
    "fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.05), ncol=3, frameon=False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"'Liquor, wine, beer' , 'Frozen foods', 'Grocery I',  'Grocery II', and 'Home and Kitchen I' seem to spike in december every year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a46112",
   "metadata": {},
   "source": [
    "After removing the entries for the years with no sales in specific categories, along with imputing the missing information of the sales for 11 categories in months 2, 4, 5, 6, 8 in 2014, the graphs show no apparent irregularities. While the produce in 2013 appears to be flat at 0, it is actually not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0dad7-b0ec-4ef8-abba-ac22f5809bcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Notice that the line for produce in 2013 appears flat. Check what is happening\n",
    "produce_2013 = data_train_filtered.query('family == \"PRODUCE\" and year == 2013 and sales != 0')\n",
    "print('There are', produce_2013.shape[0], 'rows with non-zero sales in the Produce family in the year 2013.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769dd78-7896-4be5-be58-25647359295b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Which days of the month see the most sales?\n",
    "\n",
    "# Get unique families and sort them alphabetically\n",
    "sorted_families = sorted(data_train_filtered['family'].unique())\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(13, 25))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "\n",
    "# Initialize a list to collect legend handles and labels\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, product in enumerate(sorted_families):\n",
    "    ax = fig.add_subplot(11, 3, i + 1)\n",
    "    \n",
    "    # Plot sales data for the product family\n",
    "    data_train_filtered.query('family == @product').groupby(['day']).sales.mean().plot(ax=ax, label='Sales')\n",
    "\n",
    "    plt.title(product)\n",
    "\n",
    "    # Collect the handles and labels only once (from the first subplot)\n",
    "    if i == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Create a single legend outside the subplots\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=3, frameon=False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7113a5",
   "metadata": {},
   "source": [
    "Most food, beverage, and household items seem to spike at the beginning and end of the month. Ecuadorians get paid either monthly or bi-weekly on the 15th and the last day of the month. Celebration, books, home appliances, home and kitchen I, home and kitchen II, as well as magazines do not follow this trend. Lawn and garden fmily exhibits a spike around the 13th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5df5b4-790a-4203-8408-6f29ed340fdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Why is there a spike in lawn?\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Filter the data for the specific family and year\n",
    "select = data_train_filtered.query('family == \"LAWN AND GARDEN\" and year == 2017')\n",
    "\n",
    "# Plot sales trends for each month\n",
    "for month in range(1, 9):  # Train dataset ends in august\n",
    "    monthly_data = select.query('month == @month')\n",
    "    monthly_data.groupby(['day']).sales.mean().plot(ax=ax, label=f'Month {month}')\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Daily Sales Trends for Lawn and Garden in 2017')\n",
    "ax.set_xlabel('Day of the Month')\n",
    "ax.set_ylabel('Average Sales')\n",
    "\n",
    "ax.set_xticks(range(1, 32))  # Set x-axis ticks to cover days 1 to 31\n",
    "for day in [13, 14]:\n",
    "    ax.axvline(x=day, color='red', linestyle='--', linewidth=1)\n",
    "# Add a legend\n",
    "ax.legend(title='Month')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Filter data for the specific date and family\n",
    "filtered_data = data_train_filtered.query('year == 2017 and month == 2 and day == 14 and family == \"LAWN AND GARDEN\"')\n",
    "# Group by state and calculate average sales\n",
    "grouped_by_state_feb14 = filtered_data.groupby('state')['sales'].mean().reset_index()\n",
    "\n",
    "# Filter data for the specific date and family\n",
    "filtered_data = data_train_filtered.query('year == 2017 and month == 5 and day == 13 and family == \"LAWN AND GARDEN\"')\n",
    "# Group by state and calculate average sales\n",
    "grouped_by_state_may13 = filtered_data.groupby('state')['sales'].mean().reset_index()\n",
    "\n",
    "# Merge the two grouped DataFrames on 'state'\n",
    "merged_grouped_by_state = pd.merge(grouped_by_state_feb14, grouped_by_state_may13, on='state', how='outer')\n",
    "display(merged_grouped_by_state )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e95d8",
   "metadata": {},
   "source": [
    "There are unusual entries in  Lawn and Garden family on the 13th of April and 14th of February of 2017. The sales in this family on any other day do not exceed 40. However, the sales from these days are distributed evenly throughout the stores, and thus are unlikely to be outliers due to human error. In both cases, the states with the high sales volume are the same. They are geographically neighboring to each other, which implies that the high sales could be attributed to some local event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30b2cb",
   "metadata": {},
   "source": [
    "### Data Cleaning Conculsion <br>\n",
    "Overall, the data was aleady clean, with no missing values or imbalances. <br>\n",
    "To see if it makes any difference for model training, I created two versions of the dataset: the first one is unfiltered, it has entries where sales for certain categories are 0 the whole year of 2013 and there are several months with 0 sales for certain item categories in 2014. <br>\n",
    "In the second, filtered, dataset 2013 entries with 0 sales the whole year were removed and the 0 sales months in 2014 wrere imputed by taking the mean of the other months that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d4f35-5521-47cd-b727-d647a071d9bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Which days of the week see spikes in sales?\n",
    "\n",
    "fig = plt.figure(figsize=(13, 25))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "\n",
    "# Initialize a list to collect legend handles and labels\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, product in enumerate(data_train_filtered.family.unique()):\n",
    "    ax = fig.add_subplot(11, 3, i + 1)\n",
    "    data_train_filtered.query('family==@product').groupby(['day_of_week']).sales.mean().plot(ax=ax,label=year)\n",
    "\n",
    "    plt.title(product)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f136128",
   "metadata": {},
   "source": [
    "Most shopping tends to happen on Saturdays (5) and Sundays (6), with the excpetion of alcohol, the sale of which used to be prohibited on Sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a80d8-e2ef-435a-afb7-7665cfcfb87b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize = (11,4))\n",
    "\n",
    "# Sales ranking by store\n",
    "data_train_filtered.groupby('store_nbr').sales.mean().plot(kind='bar', ax=axs[0])\n",
    "axs[0].set_title('Mean Sales by Store')\n",
    "axs[0].set_xlabel('Store Number')\n",
    "axs[0].set_ylabel('Average Sales')\n",
    "\n",
    "# Promotion items ranking by store\n",
    "data_train_filtered.groupby('store_nbr').onpromotion.mean().plot(kind='bar', ax=axs[1])\n",
    "axs[1].set_title('Mean Promotion Items by Store')\n",
    "axs[1].set_xlabel('Store Number')\n",
    "axs[1].set_ylabel('Average Number of Promotions')\n",
    "\n",
    "# Adjust the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331a673",
   "metadata": {},
   "source": [
    "From the graph above, it is evident that there is some correlation between store sales and number of items on promotion. For example, stores #44-51 have a high volume of sales accompanied by a large number of items on promotion. On the othetr hand, store #52 has both low number of discounte ditems and low sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd078f0-dccd-4ab4-81ad-92f89e467f46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# What is the correlation between Sales and Promotions\n",
    "# Group by 'store_nbr' and calculate the mean for 'onpromotion' and 'sales'\n",
    "grouped_data = data_train_filtered.groupby('store_nbr')[['onpromotion', 'sales']].mean()\n",
    "\n",
    "# Plot the scatter plot with a trend line using Seaborn's regplot\n",
    "plt.figure()\n",
    "sns.regplot(x='onpromotion', y='sales', data=grouped_data)\n",
    "\n",
    "plt.title('Promotion and Sales Relationship')\n",
    "plt.xlabel('Average Number of Promotions')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the correlation between 'sales' and 'onpromotion'\n",
    "correlation = data_train_filtered[['sales', 'onpromotion']].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"The correlation coefficient is\", correlation.loc['sales', 'onpromotion'] , \", which indicates that there  is moderate positive correlation between sales and number of promotion items\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68d87d",
   "metadata": {},
   "source": [
    "The moderate positive correlation between the average number of promotions and the average sales figure indicates that on average, for each additional promotional item that a store has, it's sales increase by roughly 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983f9d1-026d-4e2a-ae67-7613a55be804",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#How do oil prices correlate with sales?\n",
    "# Ensure 'date' is a datetime type\n",
    "data_train_filtered['date'] = pd.to_datetime(data_train_filtered['date'])\n",
    "\n",
    "# Aggregate data by week\n",
    "weekly_aggregated_data = data_train_filtered.resample('W-Mon', on='date').agg(\n",
    "    mean_sales=('sales', 'mean'),\n",
    "    avg_oil_prices=('dcoilwtico', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Create a figure and a set of subplots with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot mean sales on the first y-axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Mean Sales', color=color)\n",
    "ax1.plot(weekly_aggregated_data['date'], weekly_aggregated_data['mean_sales'], color=color, label='Mean Sales')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis for oil prices\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Average Oil Prices (dcoilwtico)', color=color)\n",
    "ax2.plot(weekly_aggregated_data['date'], weekly_aggregated_data['avg_oil_prices'], color=color, label='Average Oil Prices')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add titles and legends\n",
    "plt.title('Mean Sales and Average Oil Prices by Week')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64bf0c9",
   "metadata": {},
   "source": [
    "The figure above graphs the mean sales and oil prices over the years. Noticeably, the overall trend for the sales is positive, meaning it increase from year to year, while the oil prices decrease with a large drop around mid 2014. Despite the overall trends moving in opposite direction for these variables, during the oil price drop in 2014 and its subsequent recovery in 2015, the mean sales dwindled during that period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ec8f7",
   "metadata": {},
   "source": [
    "### Data Visualization Summary <br>\n",
    "Overall, there is strong evidence of correlation between sales and oil prices, promotions, days of the week, month, months of the year, as evidenced by the graphs in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ab298-2650-49dc-bec5-9c676a8f2e2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Which products do people stop buying when th eprice of oil goes up?\n",
    "\n",
    "# Aggregate data by date and family\n",
    "aggregated_data = data_train_filtered.groupby(['family', 'date']).agg(\n",
    "    mean_sales=('sales', 'mean'),\n",
    "    avg_oil_prices=('dcoilwtico', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Compute correlation for each family\n",
    "correlations = {}\n",
    "for family in aggregated_data['family'].unique():\n",
    "    family_data = aggregated_data[aggregated_data['family'] == family]\n",
    "    correlation = family_data[['mean_sales', 'avg_oil_prices']].corr().iloc[0, 1]\n",
    "    correlations[family] = correlation\n",
    "\n",
    "# Convert to DataFrame\n",
    "correlation_df = pd.DataFrame(list(correlations.items()), columns=['Family', 'Correlation'])\n",
    "\n",
    "# Add a column for absolute correlation values\n",
    "correlation_df['Absolute Correlation'] = correlation_df['Correlation'].abs()\n",
    "\n",
    "# Sort by absolute correlation values in descending order\n",
    "sorted_correlation_df = correlation_df.sort_values(by='Absolute Correlation', ascending=False)\n",
    "\n",
    "# Drop the Absolute Correlation column for final output\n",
    "sorted_correlation_df = sorted_correlation_df.drop(columns='Absolute Correlation')\n",
    "\n",
    "# Print sorted correlations\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "display(sorted_correlation_df)\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3643f7f",
   "metadata": {},
   "source": [
    "The table above shows the correlation between oil prices and mean sales across all stores, sorted by their absolute value. The sales of item families on the top of the list show a strong negative correlation with oil price. This means that when the oil prices increase, the  Sales of all item families go down as the price of oil rises, except for lingerie, books, ladieswear the sales of which go up. Home appliances, home care items as weel as players and electronics are bought at roughly the same rate, regardless of oil price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b4c48",
   "metadata": {},
   "source": [
    "### Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e273a-384a-4820-9887-f4fab4d07fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model with the filtered data \n",
    "\n",
    "features = ['id', 'store_nbr', 'family', 'onpromotion', 'year',\n",
    "       'month', 'day', 'day_of_week', 'day_name', 'dcoilwtico', 'holiday_type',\n",
    "       'locale', 'locale_name', 'description', 'transferred', 'city', 'state',\n",
    "       'store_type', 'cluster', 'transactions']\n",
    "\n",
    "# Encode categorical features using get_dummies\n",
    "data_train_encoded = pd.get_dummies(data_train_filtered[features], drop_first=True)\n",
    "\n",
    "\n",
    "target = 'sales'\n",
    "X = data_train_encoded\n",
    "y = data_train_filtered[target]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee0d74-946f-428c-b332-a9a83bbc88ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build model with unfiltered data\n",
    "\n",
    "features = ['id', 'store_nbr', 'family', 'onpromotion', 'year',\n",
    "       'month', 'day', 'day_of_week', 'day_name', 'dcoilwtico', 'holiday_type',\n",
    "       'locale', 'locale_name', 'description', 'transferred', 'city', 'state',\n",
    "       'store_type', 'cluster', 'transactions']\n",
    "\n",
    "data_train_encoded = pd.get_dummies(data1[features], drop_first=True)\n",
    "\n",
    "\n",
    "target = 'sales'\n",
    "X = data_train_encoded\n",
    "y = data1[target]\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model2 = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "xgb_model2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54e3f9",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfe2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the filtered test set \n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d2b90",
   "metadata": {},
   "source": [
    "Above are the results for the filtered model, removing the prolonged  and recurring 0 sales entries. This is a highly predictive model, expalining 94% of the variability in sales. I provided 4 diffrent metrics to estimate this reagression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the unfiltered test set 2\n",
    "y_pred2 = xgb_model2.predict(X_test2)\n",
    "\n",
    "\n",
    "mse2 = mean_squared_error(y_test2, y_pred2)\n",
    "r22 = r2_score(y_test2, y_pred2)\n",
    "mae2 = mean_absolute_error(y_test2, y_pred2)\n",
    "rmse2 = root_mean_squared_error(y_test2, y_pred2)\n",
    "print(f\"Mean Squared Error: {mse2:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse2:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae2:.2f}\")\n",
    "print(f\"R^2 Score: {r22:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d2f41",
   "metadata": {},
   "source": [
    "Above are the results of teh unfiltered model. The **Mean Squared Error (MSE)** of 69062.80 measures the average squared differences between predicted and actual values, penalizing larger errors. **Root Mean Squared Error (RMSE)** of 262.80 gives an interpretable error magnitude in the original units. **Mean Absolute Error (MAE)** of 79.30 reflects the average magnitude of errors without considering direction. The **R² Score** of 0.95 indicates that 95% of the variance in sales is explained by the model. These metrics provide distinct perspectives on model performance. <br>\n",
    "As you can see, this model performs even better than the filtered one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance\n",
    "xgb.plot_importance(xgb_model2, importance_type='weight', max_num_features=10)\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae83ea",
   "metadata": {},
   "source": [
    "From the graph above, it is evident that teh volume of transactions was by far the most important predictor of sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs. Predicted Sales\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test2, y_pred2, alpha=0.3)\n",
    "plt.plot([y_test2.min(), y_test2.max()], [y_test2.min(), y_test2.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a smaller portion of the errors for faster plotting\n",
    "sampled_errors = errors.sample(frac=0.1, random_state=42)\n",
    "\n",
    "sns.histplot(sampled_errors, kde=True, color='blue')\n",
    "plt.title('Error Distribution (Residuals)')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17b1cd",
   "metadata": {},
   "source": [
    "This model was not covered in class, but it is a supervised learnning algorithm based on decision trees and gradient boosting. For this model, I employed feature engineering by splitting the date data into several columns and one hot encdoing the categorical values. I also tuned the hyperparameters, manually setting the number of estimators and leraning rate. <br>\n",
    "<br>\n",
    "To solve this regression task (predicting continuous values), I trained a XGBoost model. The two iterations of this model use filtered and unfiltered data to predict sales. First, categorical features are encoded using one-hot encoding, which converts them into binary columns. The dataset is split into training and testing sets, with 80% used for training and 20% for testing. The model chosen is an XGBoost regressor (`XGBRegressor`) with 200 estimators, a learning rate of 0.1, and a maximum depth of 6. XGBoost is based on gradient boosting of decision trees to optimize performance, making it a good pick for large datasets such as this one.The model is trained on the training data  to predict the target variable, `sales`.The model achieves very high predictive power with both filtered and unfiltered datasets. In fact, the R-squared value is 0.01 higher for the unfiltered dataset, and the MSE is 11% lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff04ba1",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "<br>\n",
    "In this project, I combined data from 5 separate datasets to forecast the sales for a large supermarket chain. After performing a detailed exploratory analysis, I used features such as oil price, holiday schedule, location, as well as past sales and transactions history to train a supervised machine learning algorithm, XGboost to predict the upcoming sales. The algorithm achieved very high hpredictive power of 0.95, which means that 95% of variability in sales can be explained by this regression model.<br>\n",
    "This project highlights the importance of data preprocessing, handling missing or erroneous data, and optimizing model performance through hyperparameter tuning. Key takeaways include understanding how different evaluation metrics reveal model accuracy and error patterns, and how iterative training and analysis improve predictive outcomes in real-world datasets.<br>\n",
    "Deleting rows with 0 sales and imputing data didn't improve the model because these zero sales might hold meaningful patterns or signals related to seasonal trends, store closures, or stockouts. Removing or imputing them may have introduced noise or bias, distorting relationships in the data and reducing prediction accuracy.<br>\n",
    "Overall, this project was successful in its goal, and can be further improved. To further improve the model, one could tune hyperparameters with techniques like Grid Search or Randomized Search. Incorporate feature engineering to capture more relevant information. Use cross-validation for robust evaluation and explore advanced models or ensemble methods. Address any data quality issues and ensure feature scaling for better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
